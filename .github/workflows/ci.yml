name: Package CI

on:
  push:
    branches: [main, master, develop, development]
  pull_request:
    branches: [main, master, develop, development]
  workflow_dispatch:

jobs:
  test:
    name: Test Python ${{ matrix.python-version }} on ${{ matrix.os }}
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        include:
          # Ubuntu: Full Python version coverage (primary development platform)
          - os: ubuntu-latest
            python-version: "3.10"
            pixi-environment: "py310"
          - os: ubuntu-latest
            python-version: "3.11"
            pixi-environment: "py311"
          - os: ubuntu-latest
            python-version: "3.12"
            pixi-environment: "py312"

          # macOS: Python 3.12+ only (aligned with Hummingbot requirements)
          - os: macos-latest
            python-version: "3.12"
            pixi-environment: "py312"
    defaults:
      run:
        shell: bash

    steps:
      - uses: actions/checkout@v4

      - name: Install Pixi
        uses: prefix-dev/setup-pixi@v0.8.8
        with:
          environments: ${{ matrix.pixi-environment }}

      - name: Install project dependencies (using Pixi)
        run: |
          pixi install -e ${{ matrix.pixi-environment }}
          echo "Python version check:"
          pixi run -e ${{ matrix.pixi-environment }} python --version
          pixi run -e ${{ matrix.pixi-environment }} pip install -e . --verbose

      - name: Run unit tests with coverage
        run: |
          # Run unit tests first to avoid URL patching pollution from integration tests
          # Exclude benchmark tests from CI blocking - they're for performance monitoring only
          pixi run -e ${{ matrix.pixi-environment }} pytest tests -m "not integration and not benchmark" --cov=candles_feed --cov-report=xml:coverage.xml

      - name: Run integration tests separately
        run: |
          # Run integration tests separately to prevent URL patching pollution
          # Exclude benchmark tests from CI blocking - they're for performance monitoring only
          pixi run -e ${{ matrix.pixi-environment }} pytest tests -m "integration and not benchmark" --cov=candles_feed --cov-append

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v5
        if: matrix.os == 'ubuntu-latest' && matrix.python-version == '3.10'
        with:
          files: ./coverage.xml
          fail_ci_if_error: false

  lint:
    name: Lint and Type Check (Python ${{ matrix.python-version }})
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ["3.10", "3.11", "3.12"]
        include:
          - python-version: "3.10"
            pixi-environment: "py310"
          - python-version: "3.11"
            pixi-environment: "py311"
          - python-version: "3.12"
            pixi-environment: "py312"
    defaults:
      run:
        shell: bash

    steps:
      - uses: actions/checkout@v4

      - name: Install Pixi
        uses: prefix-dev/setup-pixi@v0.8.8
        with:
          environments: ${{ matrix.pixi-environment }}

      - name: Install project dependencies (using Pixi)
        run: |
          pixi install -e ${{ matrix.pixi-environment }}
          echo "Python version check:"
          pixi run -e ${{ matrix.pixi-environment }} python --version
          pixi run -e ${{ matrix.pixi-environment }} pip install -e . --verbose

      - name: Format and lint with ruff
        run: |
          pixi run -e ${{ matrix.pixi-environment }} ci-check

      - name: Type check with mypy
        run: |
          pixi run -e ${{ matrix.pixi-environment }} ci-hints

  benchmark:
    name: Performance Benchmarks (Non-blocking)
    runs-on: ubuntu-latest
    needs: [test]
    if: github.event_name == 'pull_request' || github.ref == 'refs/heads/main'
    continue-on-error: true  # Don't block CI if benchmarks fail
    defaults:
      run:
        shell: bash

    steps:
      - uses: actions/checkout@v4

      - name: Install Pixi
        uses: prefix-dev/setup-pixi@v0.8.8
        with:
          environments: default

      - name: Install project dependencies (using Pixi)
        run: |
          pixi install -e default
          pixi run -e default pip install -e . --verbose

      - name: Create benchmark results directory
        run: |
          mkdir -p benchmark-results

      - name: Run performance benchmarks
        run: |
          # Run performance benchmark tests for monitoring (non-blocking)
          # These tests measure performance characteristics but don't block CI
          # Capture JSON output for performance trend analysis
          pixi run -e default pytest tests -m "benchmark" -v --tb=short \
            --benchmark-json=benchmark-results/benchmark-results.json \
            --benchmark-sort=mean

      - name: Generate benchmark summary
        if: always()
        run: |
          if [ -f benchmark-results/benchmark-results.json ]; then
            echo "## Performance Benchmark Results" >> $GITHUB_STEP_SUMMARY
            echo "Benchmark completed successfully. Results stored as artifacts." >> $GITHUB_STEP_SUMMARY

            # Extract key metrics for summary
            python3 -c "import json; data=json.load(open('benchmark-results/benchmark-results.json')); benchmarks=data.get('benchmarks',[]); print(f'Total benchmarks: {len(benchmarks)}'); print(f'Fastest test: {min(benchmarks,key=lambda x:x[\"stats\"][\"mean\"])[\"name\"]} ({min(benchmarks,key=lambda x:x[\"stats\"][\"mean\"])[\"stats\"][\"mean\"]:.2f}μs)') if benchmarks else None; print(f'Slowest test: {max(benchmarks,key=lambda x:x[\"stats\"][\"mean\"])[\"name\"]} ({max(benchmarks,key=lambda x:x[\"stats\"][\"mean\"])[\"stats\"][\"mean\"]:.2f}μs)') if benchmarks else None" >> $GITHUB_STEP_SUMMARY
          else
            echo "Benchmark results file not found" >> $GITHUB_STEP_SUMMARY
          fi

      - name: Store benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results-${{ github.run_id }}
          path: |
            benchmark-results/
            tests/performance/
          retention-days: 30

  # Summary job that reports overall workflow status
  ci-status:
    name: Package CI
    runs-on: ubuntu-latest
    needs: [test, lint, benchmark]
    if: always()
    steps:
      - name: Report CI Status
        run: |
          # Check if all required jobs passed
          if [[ "${{ needs.test.result }}" == "success" && "${{ needs.lint.result }}" == "success" ]]; then
            echo "✅ All Package CI jobs passed"
            exit 0
          else
            echo "❌ Some Package CI jobs failed"
            echo "Test result: ${{ needs.test.result }}"
            echo "Lint result: ${{ needs.lint.result }}"
            echo "Benchmark result: ${{ needs.benchmark.result }}"
            exit 1
          fi
